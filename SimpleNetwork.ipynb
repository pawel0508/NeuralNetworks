{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled55.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPo+jOYEYLrMnJvibiUPAXy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawel0508/NeuralNetworks/blob/main/SimpleNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRhGGuXMjVxl"
      },
      "source": [
        "### Implementacja prostej sieci neuronowej\n",
        "\n",
        "##### $Kroki:$\n",
        "1. Zainicjowanie paramterów sieci.\n",
        "2. Propagacja w przód.\n",
        "3. Obliczenie błędu predykcji.\n",
        "4. Propagacja wsteczna $(uczenie$ $modelu)$.\n",
        "5. Test działania modelu.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7s3xWxGjLlG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.array([1.4, 0.7])\n",
        "y_true = np.array([1.8])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqA7I38zkGXN"
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "  W1 = np.random.rand(n_h, n_x)\n",
        "  W2 = np.random.rand(n_h, n_y)\n",
        "  return W1, W2"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq2ugDlUs-Qz"
      },
      "source": [
        "def forward_propagation(X, W1, W2):\n",
        "  H1 = np.dot(X, W1)\n",
        "  y_pred = np.dot(H1, W2)\n",
        "  return H1, y_pred"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6pOioZWtxZ0"
      },
      "source": [
        "def calculate_error(y_pred, y_true):\n",
        "  return y_pred - y_true"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-DCocUmt-De"
      },
      "source": [
        "def predict(X, W1, W2):\n",
        "  _, y_pred = forward_propagation(X, W1, W2)\n",
        "  return y_pred[0]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHs3NZrKuNmy"
      },
      "source": [
        "def backpropagation(X, W1, W2, learning_rate, iters=1000, precision = 0.0000001):\n",
        "\n",
        "    H1, y_pred = forward_propagation(X, W1, W2)\n",
        "    train_loss = []\n",
        "\n",
        "    for i in range(iters):\n",
        "        error = calculate_error(y_pred, y_true)\n",
        "        W2 = W2 - learning_rate * error * H1.T \n",
        "        W1 = W1 - learning_rate * error * np.dot(X.T, W2.T)\n",
        "\n",
        "        y_pred = predict(X, W1, W2)\n",
        "        print(f'Iter #{i}: y_pred {y_pred}: loss: {abs(calculate_error(y_pred, y_true[0]))}')\n",
        "        train_loss.append(abs(calculate_error(y_pred, y_true[0])))\n",
        "\n",
        "        if abs(error) < precision:\n",
        "            break\n",
        "\n",
        "    return W1, W2, train_loss\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFvSRqTeyyHx"
      },
      "source": [
        "def build_model():\n",
        "\n",
        "    W1, W2 = initialize_parameters(2, 2, 1)\n",
        "    \n",
        "    W1, W2, train_loss = backpropagation(X, W1, W2, 0.01)\n",
        "\n",
        "    model = {'W1': W1, 'W2': W2, 'train_loss': train_loss}\n",
        "\n",
        "    return model"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5Bu7RbMy3GW",
        "outputId": "91caa4f8-c28e-491f-c169-34aa69872e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter #0: y_pred 1.6742248544397074: loss: 0.12577514556029268\n",
            "Iter #1: y_pred 1.6808646849088025: loss: 0.11913531509119757\n",
            "Iter #2: y_pred 1.6871660466940408: loss: 0.11283395330595924\n",
            "Iter #3: y_pred 1.693144947187691: loss: 0.10685505281230911\n",
            "Iter #4: y_pred 1.6988167662184652: loss: 0.10118323378153482\n",
            "Iter #5: y_pred 1.7041962667506818: loss: 0.09580373324931823\n",
            "Iter #6: y_pred 1.7092976071109702: loss: 0.09070239288902981\n",
            "Iter #7: y_pred 1.7141343544911012: loss: 0.08586564550889886\n",
            "Iter #8: y_pred 1.718719499499326: loss: 0.08128050050067404\n",
            "Iter #9: y_pred 1.7230654715550362: loss: 0.07693452844496385\n",
            "Iter #10: y_pred 1.7271841549426286: loss: 0.07281584505737149\n",
            "Iter #11: y_pred 1.7310869053600926: loss: 0.06891309463990747\n",
            "Iter #12: y_pred 1.7347845668160795: loss: 0.06521543318392053\n",
            "Iter #13: y_pred 1.7382874887460855: loss: 0.061712511253914526\n",
            "Iter #14: y_pred 1.7416055432338824: loss: 0.05839445676611765\n",
            "Iter #15: y_pred 1.744748142238588: loss: 0.05525185776141206\n",
            "Iter #16: y_pred 1.7477242547407283: loss: 0.0522757452592717\n",
            "Iter #17: y_pred 1.7505424237325011: loss: 0.049457576267498915\n",
            "Iter #18: y_pred 1.7532107829881463: loss: 0.04678921701185379\n",
            "Iter #19: y_pred 1.7557370735600222: loss: 0.04426292643997787\n",
            "Iter #20: y_pred 1.758128659954706: loss: 0.041871340045294136\n",
            "Iter #21: y_pred 1.7603925459512528: loss: 0.039607454048747215\n",
            "Iter #22: y_pred 1.7625353900307374: loss: 0.0374646099692626\n",
            "Iter #23: y_pred 1.7645635203924346: loss: 0.03543647960756546\n",
            "Iter #24: y_pred 1.7664829495375036: loss: 0.03351705046249642\n",
            "Iter #25: y_pred 1.7682993884059404: loss: 0.03170061159405968\n",
            "Iter #26: y_pred 1.7700182600568368: loss: 0.02998173994316322\n",
            "Iter #27: y_pred 1.7716447128857606: loss: 0.028355287114239403\n",
            "Iter #28: y_pred 1.7731836333763453: loss: 0.02681636662365472\n",
            "Iter #29: y_pred 1.7746396583860131: loss: 0.025360341613986925\n",
            "Iter #30: y_pred 1.776017186968223: loss: 0.02398281303177696\n",
            "Iter #31: y_pred 1.777320391735701: loss: 0.022679608264299045\n",
            "Iter #32: y_pred 1.7785532297709208: loss: 0.02144677022907926\n",
            "Iter #33: y_pred 1.7797194530915763: loss: 0.020280546908423736\n",
            "Iter #34: y_pred 1.780822618680037: loss: 0.01917738131996294\n",
            "Iter #35: y_pred 1.781866098086802: loss: 0.01813390191319808\n",
            "Iter #36: y_pred 1.782853086618787: loss: 0.01714691338121299\n",
            "Iter #37: y_pred 1.7837866121239168: loss: 0.016213387876083285\n",
            "Iter #38: y_pred 1.784669543384002: loss: 0.015330456615997967\n",
            "Iter #39: y_pred 1.7855045981282296: loss: 0.014495401871770452\n",
            "Iter #40: y_pred 1.7862943506798281: loss: 0.013705649320171931\n",
            "Iter #41: y_pred 1.7870412392486292: loss: 0.012958760751370813\n",
            "Iter #42: y_pred 1.787747572882267: loss: 0.012252427117733111\n",
            "Iter #43: y_pred 1.788415538088754: loss: 0.011584461911245958\n",
            "Iter #44: y_pred 1.7890472051430695: loss: 0.01095279485693057\n",
            "Iter #45: y_pred 1.7896445340902472: loss: 0.010355465909752892\n",
            "Iter #46: y_pred 1.7902093804572659: loss: 0.009790619542734191\n",
            "Iter #47: y_pred 1.7907435006858063: loss: 0.0092564993141937\n",
            "Iter #48: y_pred 1.791248557297675: loss: 0.008751442702324974\n",
            "Iter #49: y_pred 1.7917261238044135: loss: 0.008273876195586505\n",
            "Iter #50: y_pred 1.7921776893722938: loss: 0.00782231062770622\n",
            "Iter #51: y_pred 1.7926046632535833: loss: 0.007395336746416792\n",
            "Iter #52: y_pred 1.7930083789946156: loss: 0.0069916210053844186\n",
            "Iter #53: y_pred 1.7933900984308777: loss: 0.006609901569122334\n",
            "Iter #54: y_pred 1.7937510154789524: loss: 0.006248984521047607\n",
            "Iter #55: y_pred 1.7940922597348195: loss: 0.005907740265180594\n",
            "Iter #56: y_pred 1.7944148998876615: loss: 0.005585100112338592\n",
            "Iter #57: y_pred 1.7947199469579687: loss: 0.005280053042031385\n",
            "Iter #58: y_pred 1.795008357368397: loss: 0.0049916426316030105\n",
            "Iter #59: y_pred 1.7952810358554891: loss: 0.004718964144510895\n",
            "Iter #60: y_pred 1.7955388382300281: loss: 0.004461161769971911\n",
            "Iter #61: y_pred 1.795782573993474: loss: 0.0042174260065259705\n",
            "Iter #62: y_pred 1.7960130088176156: loss: 0.003986991182384436\n",
            "Iter #63: y_pred 1.7962308668942402: loss: 0.003769133105759881\n",
            "Iter #64: y_pred 1.7964368331613418: loss: 0.003563166838658205\n",
            "Iter #65: y_pred 1.796631555412089: loss: 0.003368444587910968\n",
            "Iter #66: y_pred 1.796815646292478: loss: 0.003184353707522014\n",
            "Iter #67: y_pred 1.7969896851933411: loss: 0.0030103148066589203\n",
            "Iter #68: y_pred 1.7971542200420993: loss: 0.002845779957900696\n",
            "Iter #69: y_pred 1.7973097689994069: loss: 0.0026902310005931795\n",
            "Iter #70: y_pred 1.7974568220655736: loss: 0.002543177934426488\n",
            "Iter #71: y_pred 1.7975958426014405: loss: 0.002404157398559592\n",
            "Iter #72: y_pred 1.7977272687681283: loss: 0.002272731231871772\n",
            "Iter #73: y_pred 1.7978515148898857: loss: 0.00214848511011434\n",
            "Iter #74: y_pred 1.7979689727440455: loss: 0.0020310272559544984\n",
            "Iter #75: y_pred 1.7980800127819012: loss: 0.0019199872180988287\n",
            "Iter #76: y_pred 1.7981849852841196: loss: 0.0018150147158804142\n",
            "Iter #77: y_pred 1.7982842214541437: loss: 0.0017157785458563612\n",
            "Iter #78: y_pred 1.7983780344528348: loss: 0.001621965547165205\n",
            "Iter #79: y_pred 1.7984667203774705: loss: 0.001533279622529582\n",
            "Iter #80: y_pred 1.7985505591880335: loss: 0.001449440811966518\n",
            "Iter #81: y_pred 1.7986298155835871: loss: 0.0013701844164129007\n",
            "Iter #82: y_pred 1.798704739831392: loss: 0.0012952601686080367\n",
            "Iter #83: y_pred 1.798775568551266: loss: 0.0012244314487339647\n",
            "Iter #84: y_pred 1.7988425254575853: loss: 0.0011574745424147714\n",
            "Iter #85: y_pred 1.7989058220611762: loss: 0.001094177938823826\n",
            "Iter #86: y_pred 1.7989656583332423: loss: 0.0010343416667577543\n",
            "Iter #87: y_pred 1.799022223333366: loss: 0.0009777766666341225\n",
            "Iter #88: y_pred 1.7990756958034935: loss: 0.0009243041965065757\n",
            "Iter #89: y_pred 1.799126244729744: loss: 0.0008737552702560158\n",
            "Iter #90: y_pred 1.799174029873755: loss: 0.0008259701262449681\n",
            "Iter #91: y_pred 1.7992192022752145: loss: 0.0007807977247855646\n",
            "Iter #92: y_pred 1.7992619047271206: loss: 0.000738095272879491\n",
            "Iter #93: y_pred 1.7993022722252405: loss: 0.0006977277747595156\n",
            "Iter #94: y_pred 1.7993404323931577: loss: 0.0006595676068423817\n",
            "Iter #95: y_pred 1.799376505884229: loss: 0.0006234941157710061\n",
            "Iter #96: y_pred 1.799410606761691: loss: 0.0005893932383089773\n",
            "Iter #97: y_pred 1.799442842858101: loss: 0.0005571571418989674\n",
            "Iter #98: y_pred 1.7994733161152252: loss: 0.0005266838847748367\n",
            "Iter #99: y_pred 1.799502122905439: loss: 0.0004978770945609501\n",
            "Iter #100: y_pred 1.7995293543356317: loss: 0.0004706456643683854\n",
            "Iter #101: y_pred 1.7995550965345695: loss: 0.00044490346543057946\n",
            "Iter #102: y_pred 1.799579430924608: loss: 0.0004205690753920077\n",
            "Iter #103: y_pred 1.7996024344786088: loss: 0.0003975655213912521\n",
            "Iter #104: y_pred 1.7996241799628567: loss: 0.00037582003714331513\n",
            "Iter #105: y_pred 1.7996447361667398: loss: 0.00035526383326023314\n",
            "Iter #106: y_pred 1.799664168119914: loss: 0.00033583188008612375\n",
            "Iter #107: y_pred 1.799682537297625: loss: 0.0003174627023749821\n",
            "Iter #108: y_pred 1.7996999018148385: loss: 0.00030009818516152365\n",
            "Iter #109: y_pred 1.7997163166097823: loss: 0.00028368339021778155\n",
            "Iter #110: y_pred 1.799731833617475: loss: 0.0002681663825250258\n",
            "Iter #111: y_pred 1.7997465019337928: loss: 0.00025349806620722504\n",
            "Iter #112: y_pred 1.7997603679705827: loss: 0.00023963202941734707\n",
            "Iter #113: y_pred 1.7997734756023094: loss: 0.0002265243976906639\n",
            "Iter #114: y_pred 1.799785866304701: loss: 0.0002141336952989903\n",
            "Iter #115: y_pred 1.7997975792858272: loss: 0.00020242071417286844\n",
            "Iter #116: y_pred 1.7998086516100218: loss: 0.000191348389978252\n",
            "Iter #117: y_pred 1.7998191183150372: loss: 0.00018088168496288581\n",
            "Iter #118: y_pred 1.7998290125228067: loss: 0.00017098747719335172\n",
            "Iter #119: y_pred 1.7998383655441539: loss: 0.0001616344558461602\n",
            "Iter #120: y_pred 1.799847206977787: loss: 0.0001527930222129381\n",
            "Iter #121: y_pred 1.799855564803882: loss: 0.00014443519611795352\n",
            "Iter #122: y_pred 1.7998634654725627: loss: 0.00013653452743733752\n",
            "Iter #123: y_pred 1.7998709339875358: loss: 0.00012906601246420735\n",
            "Iter #124: y_pred 1.7998779939851666: loss: 0.00012200601483347562\n",
            "Iter #125: y_pred 1.7998846678092297: loss: 0.00011533219077031198\n",
            "Iter #126: y_pred 1.7998909765815765: loss: 0.00010902341842355945\n",
            "Iter #127: y_pred 1.799896940268942: loss: 0.00010305973105806387\n",
            "Iter #128: y_pred 1.7999025777460995: loss: 9.742225390052539e-05\n",
            "Iter #129: y_pred 1.799907906855566: loss: 9.209314443414662e-05\n",
            "Iter #130: y_pred 1.7999129444640425: loss: 8.705553595755866e-05\n",
            "Iter #131: y_pred 1.7999177065157714: loss: 8.229348422861271e-05\n",
            "Iter #132: y_pred 1.7999222080829755: loss: 7.779191702450561e-05\n",
            "Iter #133: y_pred 1.7999264634135437: loss: 7.353658645636862e-05\n",
            "Iter #134: y_pred 1.799930485976104: loss: 6.951402389598904e-05\n",
            "Iter #135: y_pred 1.7999342885026408: loss: 6.57114973592332e-05\n",
            "Iter #136: y_pred 1.799937883028775: loss: 6.211697122493476e-05\n",
            "Iter #137: y_pred 1.7999412809318465: loss: 5.8719068153578746e-05\n",
            "Iter #138: y_pred 1.7999444929669128: loss: 5.5507033087209834e-05\n",
            "Iter #139: y_pred 1.79994752930078: loss: 5.247069921998637e-05\n",
            "Iter #140: y_pred 1.7999503995441737: loss: 4.9600455826359635e-05\n",
            "Iter #141: y_pred 1.799953112782151: loss: 4.688721784895655e-05\n",
            "Iter #142: y_pred 1.799955677602848: loss: 4.432239715201902e-05\n",
            "Iter #143: y_pred 1.7999581021246587: loss: 4.189787534136791e-05\n",
            "Iter #144: y_pred 1.7999603940219229: loss: 3.960597807717292e-05\n",
            "Iter #145: y_pred 1.799962560549215: loss: 3.743945078493738e-05\n",
            "Iter #146: y_pred 1.799964608564301: loss: 3.539143569897263e-05\n",
            "Iter #147: y_pred 1.799966544549841: loss: 3.345545015909224e-05\n",
            "Iter #148: y_pred 1.7999683746339044: loss: 3.162536609568889e-05\n",
            "Iter #149: y_pred 1.799970104609366: loss: 2.9895390634138153e-05\n",
            "Iter #150: y_pred 1.7999717399522364: loss: 2.826004776368407e-05\n",
            "Iter #151: y_pred 1.7999732858389965: loss: 2.671416100352708e-05\n",
            "Iter #152: y_pred 1.7999747471629766: loss: 2.525283702348169e-05\n",
            "Iter #153: y_pred 1.7999761285498461: loss: 2.387145015392278e-05\n",
            "Iter #154: y_pred 1.7999774343722525: loss: 2.2565627747495043e-05\n",
            "Iter #155: y_pred 1.7999786687636614: loss: 2.1331236338628656e-05\n",
            "Iter #156: y_pred 1.7999798356314378: loss: 2.0164368562225476e-05\n",
            "Iter #157: y_pred 1.7999809386692152: loss: 1.9061330784886366e-05\n",
            "Iter #158: y_pred 1.7999819813685831: loss: 1.8018631416927278e-05\n",
            "Iter #159: y_pred 1.7999829670301417: loss: 1.703296985833269e-05\n",
            "Iter #160: y_pred 1.7999838987739472: loss: 1.6101226052889217e-05\n",
            "Iter #161: y_pred 1.7999847795493862: loss: 1.5220450613862013e-05\n",
            "Iter #162: y_pred 1.799985612144513: loss: 1.4387855487019152e-05\n",
            "Iter #163: y_pred 1.7999863991948721: loss: 1.3600805127911286e-05\n",
            "Iter #164: y_pred 1.7999871431918428: loss: 1.285680815721335e-05\n",
            "Iter #165: y_pred 1.7999878464905226: loss: 1.2153509477474955e-05\n",
            "Iter #166: y_pred 1.7999885113171832: loss: 1.148868281686255e-05\n",
            "Iter #167: y_pred 1.799989139776317: loss: 1.0860223683017978e-05\n",
            "Iter #168: y_pred 1.7999897338572977: loss: 1.026614270238646e-05\n",
            "Iter #169: y_pred 1.7999902954406783: loss: 9.704559321699335e-06\n",
            "Iter #170: y_pred 1.7999908263041424: loss: 9.173695857622732e-06\n",
            "Iter #171: y_pred 1.799991328128132: loss: 8.671871868148884e-06\n",
            "Iter #172: y_pred 1.7999918025011667: loss: 8.197498833295569e-06\n",
            "Iter #173: y_pred 1.7999922509248714: loss: 7.749075128682392e-06\n",
            "Iter #174: y_pred 1.7999926748187298: loss: 7.325181270223524e-06\n",
            "Iter #175: y_pred 1.7999930755245774: loss: 6.924475422609433e-06\n",
            "Iter #176: y_pred 1.799993454310849: loss: 6.545689151149503e-06\n",
            "Iter #177: y_pred 1.7999938123765933: loss: 6.187623406761489e-06\n",
            "Iter #178: y_pred 1.7999941508552697: loss: 5.849144730341038e-06\n",
            "Iter #179: y_pred 1.7999944708183353: loss: 5.5291816647429215e-06\n",
            "Iter #180: y_pred 1.7999947732786352: loss: 5.226721364826048e-06\n",
            "Iter #181: y_pred 1.7999950591936116: loss: 4.940806388464836e-06\n",
            "Iter #182: y_pred 1.799995329468332: loss: 4.670531668082845e-06\n",
            "Iter #183: y_pred 1.7999955849583542: loss: 4.415041645833284e-06\n",
            "Iter #184: y_pred 1.7999958264724358: loss: 4.173527564210744e-06\n",
            "Iter #185: y_pred 1.7999960547750944: loss: 3.945224905654854e-06\n",
            "Iter #186: y_pred 1.7999962705890262: loss: 3.7294109738184034e-06\n",
            "Iter #187: y_pred 1.7999964745973933: loss: 3.525402606729955e-06\n",
            "Iter #188: y_pred 1.79999666744599: loss: 3.332554010082589e-06\n",
            "Iter #189: y_pred 1.7999968497452812: loss: 3.1502547188644314e-06\n",
            "Iter #190: y_pred 1.7999970220723402: loss: 2.9779276597974302e-06\n",
            "Iter #191: y_pred 1.7999971849726728: loss: 2.8150273272409265e-06\n",
            "Iter #192: y_pred 1.7999973389619435: loss: 2.6610380565728065e-06\n",
            "Iter #193: y_pred 1.7999974845276085: loss: 2.5154723914955213e-06\n",
            "Iter #194: y_pred 1.7999976221304614: loss: 2.377869538605637e-06\n",
            "Iter #195: y_pred 1.7999977522060864: loss: 2.247793913667806e-06\n",
            "Iter #196: y_pred 1.7999978751662427: loss: 2.1248337573887e-06\n",
            "Iter #197: y_pred 1.7999979914001631: loss: 2.00859983690016e-06\n",
            "Iter #198: y_pred 1.79999810127579: loss: 1.898724210080971e-06\n",
            "Iter #199: y_pred 1.7999982051409382: loss: 1.7948590618210858e-06\n",
            "Iter #200: y_pred 1.7999983033243956: loss: 1.6966756044567433e-06\n",
            "Iter #201: y_pred 1.7999983961369646: loss: 1.6038630354930916e-06\n",
            "Iter #202: y_pred 1.7999984838724477: loss: 1.5161275523922768e-06\n",
            "Iter #203: y_pred 1.7999985668085732: loss: 1.4331914268694845e-06\n",
            "Iter #204: y_pred 1.7999986452078782: loss: 1.3547921218215464e-06\n",
            "Iter #205: y_pred 1.7999987193185387: loss: 1.2806814613242068e-06\n",
            "Iter #206: y_pred 1.7999987893751543: loss: 1.2106248457044444e-06\n",
            "Iter #207: y_pred 1.7999988555994917: loss: 1.1444005083571795e-06\n",
            "Iter #208: y_pred 1.7999989182011866: loss: 1.0817988134181888e-06\n",
            "Iter #209: y_pred 1.7999989773784064: loss: 1.0226215936270933e-06\n",
            "Iter #210: y_pred 1.7999990333184779: loss: 9.666815221631708e-07\n",
            "Iter #211: y_pred 1.799999086198483: loss: 9.138015171217262e-07\n",
            "Iter #212: y_pred 1.7999991361858139: loss: 8.638141861805337e-07\n",
            "Iter #213: y_pred 1.799999183438708: loss: 8.165612921384735e-07\n",
            "Iter #214: y_pred 1.7999992281067458: loss: 7.718932542033485e-07\n",
            "Iter #215: y_pred 1.7999992703313257: loss: 7.296686743707426e-07\n",
            "Iter #216: y_pred 1.7999993102461112: loss: 6.897538888939181e-07\n",
            "Iter #217: y_pred 1.7999993479774536: loss: 6.52022546399067e-07\n",
            "Iter #218: y_pred 1.7999993836447934: loss: 6.163552066507094e-07\n",
            "Iter #219: y_pred 1.7999994173610365: loss: 5.826389635199547e-07\n",
            "Iter #220: y_pred 1.7999994492329134: loss: 5.507670866045089e-07\n",
            "Iter #221: y_pred 1.7999994793613154: loss: 5.206386846090538e-07\n",
            "Iter #222: y_pred 1.7999995078416153: loss: 4.921583847128375e-07\n",
            "Iter #223: y_pred 1.7999995347639686: loss: 4.6523603147718973e-07\n",
            "Iter #224: y_pred 1.7999995602135985: loss: 4.3978640151820514e-07\n",
            "Iter #225: y_pred 1.799999584271067: loss: 4.15728933056414e-07\n",
            "Iter #226: y_pred 1.799999607012529: loss: 3.92987471009576e-07\n",
            "Iter #227: y_pred 1.7999996285099737: loss: 3.714900262963283e-07\n",
            "Iter #228: y_pred 1.7999996488314522: loss: 3.5116854779637663e-07\n",
            "Iter #229: y_pred 1.7999996680412924: loss: 3.319587076333619e-07\n",
            "Iter #230: y_pred 1.7999996862003045: loss: 3.137996955615563e-07\n",
            "Iter #231: y_pred 1.7999997033659707: loss: 2.966340293397707e-07\n",
            "Iter #232: y_pred 1.7999997195926305: loss: 2.8040736954615397e-07\n",
            "Iter #233: y_pred 1.799999734931649: loss: 2.650683510463381e-07\n",
            "Iter #234: y_pred 1.7999997494315831: loss: 2.505684169040734e-07\n",
            "Iter #235: y_pred 1.7999997631383329: loss: 2.3686166716885282e-07\n",
            "Iter #236: y_pred 1.7999997760952875: loss: 2.2390471254851718e-07\n",
            "Iter #237: y_pred 1.799999788343463: loss: 2.1165653696364473e-07\n",
            "Iter #238: y_pred 1.799999799921631: loss: 2.0007836898372489e-07\n",
            "Iter #239: y_pred 1.799999810866443: loss: 1.8913355703809032e-07\n",
            "Iter #240: y_pred 1.7999998212125448: loss: 1.7878745528498996e-07\n",
            "Iter #241: y_pred 1.799999830992688: loss: 1.6900731214519737e-07\n",
            "Iter #242: y_pred 1.7999998402378314: loss: 1.5976216860558168e-07\n",
            "Iter #243: y_pred 1.799999848977242: loss: 1.5102275807699073e-07\n",
            "Iter #244: y_pred 1.7999998572385831: loss: 1.4276141691027533e-07\n",
            "Iter #245: y_pred 1.7999998650480076: loss: 1.349519924698228e-07\n",
            "Iter #246: y_pred 1.7999998724302355: loss: 1.2756976452976687e-07\n",
            "Iter #247: y_pred 1.799999879408637: loss: 1.2059136311748375e-07\n",
            "Iter #248: y_pred 1.7999998860053008: loss: 1.1399469923567551e-07\n",
            "Iter #249: y_pred 1.79999989224111: loss: 1.0775889003333816e-07\n",
            "Iter #250: y_pred 1.7999998981358043: loss: 1.0186419574509387e-07\n",
            "Iter #251: y_pred 1.7999999037080436: loss: 9.629195640847854e-08\n",
            "Iter #252: y_pred 1.7999999089754664: loss: 9.102453368825536e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBsqaPQ0zk11"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ystnobf61cIr",
        "outputId": "07168490-6bc9-4628-a93d-b9814e5a51cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "loss = pd.DataFrame({'train_loss': model['train_loss']})\n",
        "loss = loss.reset_index().rename(columns={'index': 'iter'})\n",
        "loss['iter'] += 1\n",
        "loss.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.322249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.279680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.237540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.195837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.154590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iter  train_loss\n",
              "0     1    1.322249\n",
              "1     2    1.279680\n",
              "2     3    1.237540\n",
              "3     4    1.195837\n",
              "4     5    1.154590"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsnOMwcR2QnA",
        "outputId": "3eb5194c-e8b2-4e47-ea31-50bffe51e0a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=loss['iter'], y=loss['train_loss'], mode='markers+lines', line = dict(color = 'darkred')))\n",
        "fig.update_layout(template = 'simple_white', title = 'Loss function', width = 900, height = 550, xaxis_title = 'iter', yaxis_title = 'train loss')\n",
        "fig.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9620f287-af20-433c-9c4e-d6f1391fdf61\" class=\"plotly-graph-div\" style=\"height:550px; width:900px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9620f287-af20-433c-9c4e-d6f1391fdf61\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9620f287-af20-433c-9c4e-d6f1391fdf61',\n",
              "                        [{\"line\": {\"color\": \"darkred\"}, \"mode\": \"markers+lines\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269], \"y\": [1.3222491758193113, 1.2796802835004126, 1.2375396912201282, 1.195836910115009, 1.1545903528778616, 1.113825733183146, 1.0735745588263415, 1.0338727210671392, 0.9947591840763658, 0.9562747789254677, 0.9184611063799996, 0.8813595520160398, 0.8450104160166874, 0.8094521585625345, 0.7747207601317851, 0.7408491943849318, 0.7078670097213875, 0.6758000141389238, 0.6446700567600165, 0.6144948983526191, 0.5852881623892925, 0.5570593576652942, 0.5298139632270482, 0.5035535663304791, 0.47827604432913984, 0.45397578175447695, 0.4306439143616634, 0.4082685925399354, 0.38683525719291034, 0.3663269219508194, 0.34672445635499227, 0.32800686543114455, 0.31015156182202497, 0.29313462736591345, 0.27693106167339354, 0.2615150158626227, 0.2468600101579752, 0.232939134536301, 0.21972523201933236, 0.2071910645619004, 0.19530946177702568, 0.1840534529750013, 0.1733963831792178, 0.16331201392216, 0.15377460972605572, 0.14475901123953738, 0.1362406960394622, 0.12819582812054886, 0.12060129708913325, 0.11343474805508191, 0.10667460318125088, 0.10030007580592382, 0.09429117800304665, 0.08862872239001085, 0.08329431893515449, 0.07827036745851967, 0.07354004646100076, 0.06908729885980147, 0.06489681515284218, 0.06095401448196025, 0.057245024014835666, 0.05375665701877552, 0.050476389955990575, 0.04739233888978078, 0.04449323545417161, 0.04176840260586734, 0.03920773034683789, 0.036801651578276307, 0.034541118221875555, 0.03241757772221532, 0.03042295002432671, 0.028549605103036946, 0.026790341105274562, 0.025138363153021093, 0.023587262842768242, 0.022130998467092544, 0.020763875975101653, 0.019480530680898278, 0.018275909722715422, 0.017145255269879422, 0.016084088470153635, 0.015088194126174015, 0.014153606086547033, 0.013276593334619502, 0.012453646755933745, 0.01168146656378144, 0.010956950361133533, 0.010277181816358505, 0.009639419929637905, 0.009041088866659885, 0.008479768336126181, 0.007953184487675458, 0.0074592013070977625, 0.006995812486051856, 0.006561133743990144, 0.006153395580522192, 0.005770936437060614, 0.00541219624725553, 0.00507571035640364, 0.0047601037907423205, 0.00446408585826763, 0.004186445063451005, 0.003926044318976585, 0.003681816438350527, 0.003452759893960655, 0.00323793482588397, 0.0030364592874383423, 0.0028475057141397198, 0.002670297603420302, 0.0025041063930617558, 0.002348248526952812, 0.002202082697345231, 0.00206500725337877, 0.0019364577661715732, 0.001815904741311103, 0.0017028514700812014, 0.0015968320112278445, 0.0014974092955390983, 0.0014041733459297934, 0.0013167396061577463, 0.0012347473716578516, 0.001157858316390703, 0.0010857551099228147, 0.0010181401193036788, 0.000954734190625306, 0.000895275505438553, 0.0008395185074940859, 0.0007872328955313979, 0.0007382026781053153, 0.0006922252866650158, 0.0006491107433419518, 0.0006086808800898069, 0.0005707686060512085, 0.0005352172201884553, 0.000501879766407809, 0.00047061842856810365, 0.0004413039629240778, 0.00041381516570448973, 0.00038803837366274685, 0.0003638669955674523, 0.00034120107273083455, 0.0003199468667778316, 0.00030001647298405487, 0.00028132745759368305, 0.00026380251764512863, 0.0002473691619064855, 0.00023195941161890943, 0.00021750951982202338, 0.00020395970810938024, 0.00019125391973928707, 0.00017933958807714312, 0.00016816741943781466, 0.0001576911894132227, 0.00014786755186846356, 0.0001386558598048815, 0.00013001799736933606, 0.00012191822230245286, 0.0001143230181879229, 0.00010720095589400458, 0.00010052256363102252, 9.426020509173405e-05, 8.838796517784964e-05, 8.288154283087046e-05, 7.77181505371427e-05, 7.287642008324546e-05, 6.833631417824115e-05, 6.40790435753047e-05, 6.0086989353669296e-05, 5.6343630036481684e-05, 5.2833473246804985e-05, 4.954199161821826e-05, 4.645556269378126e-05, 4.356141256933732e-05, 4.084756304334469e-05, 3.830278205874116e-05, 3.5916537230340495e-05, 3.367895226591422e-05, 3.158076610065841e-05, 2.9613294575847604e-05, 2.7768394506688665e-05, 2.603842998460948e-05, 2.4416240779645548e-05, 2.289511271036382e-05, 2.1468749859865355e-05, 2.013124852062731e-05, 1.88770727647114e-05, 1.7701031530981126e-05, 1.6598257146727136e-05, 1.5564185179339773e-05, 1.4594535547196585e-05, 1.368529480605396e-05, 1.283269954099886e-05, 1.2033220788021382e-05, 1.1283549431029272e-05, 1.0580582503694203e-05, 9.921410352164983e-06, 9.303304585595029e-06, 8.723706790059182e-06, 8.180217937026057e-06, 7.670588453079219e-06, 7.192708912251433e-06, 6.744601301900133e-06, 6.324410839031458e-06, 5.93039829110964e-06, 5.560932778037042e-06, 5.214485024440663e-06, 4.889621024739554e-06, 4.584996113221607e-06, 4.2993493949428085e-06, 4.0314985290113015e-06, 3.7803348338361076e-06, 3.5448186974651463e-06, 3.3239752756930585e-06, 3.116890457954824e-06, 2.922707080799114e-06, 2.740621383834352e-06, 2.5698796786155498e-06, 2.4097752331364575e-06, 2.2596453439494013e-06, 2.1188685945805474e-06, 1.9868622822549753e-06, 1.8630800064922681e-06, 1.747009408026301e-06, 1.6381700480572192e-06, 1.5361114178435997e-06, 1.4404110757482158e-06, 1.3506728990808625e-06, 1.2665254434107709e-06, 1.187620405351808e-06, 1.1136311799386789e-06, 1.0442515105957284e-06, 9.791942210402027e-07, 9.181900255672559e-07, 8.609864143860335e-07, 8.073466106761629e-07, 7.570485871521981e-07, 7.098841503516695e-07, 6.656580762154363e-07, 6.241873038437262e-07, 5.853001774358546e-07, 5.488357344152206e-07, 5.146430408498048e-07, 4.825805659525173e-07, 4.5251559588344037e-07, 4.243236855216992e-07, 3.978881426558445e-07, 3.7309954525888145e-07, 3.4985528740705263e-07, 3.280591565069102e-07, 3.0762093339298247e-07, 2.88456020181016e-07, 2.704850887713661e-07, 2.536337537772937e-07, 2.378322638829644e-07, 2.2301521362955157e-07, 2.091212716326396e-07, 1.9609292789546373e-07, 1.8387625577709343e-07, 1.724206872832923e-07, 1.6167880523276779e-07, 1.5160614741382972e-07, 1.421610202889667e-07, 1.33304328242545e-07, 1.249994119323361e-07, 1.1721189574487312e-07, 1.0990954479872528e-07, 1.0306213327204716e-07, 9.664131828124312e-08, 9.062052330754966e-08]}],\n",
              "                        {\"height\": 550, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"rgb(36,36,36)\"}, \"error_y\": {\"color\": \"rgb(36,36,36)\"}, \"marker\": {\"line\": {\"color\": \"white\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"white\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"rgb(36,36,36)\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"rgb(36,36,36)\"}, \"baxis\": {\"endlinecolor\": \"rgb(36,36,36)\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"rgb(36,36,36)\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"line\": {\"color\": \"white\", \"width\": 0.6}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}, \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"rgb(237,237,237)\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"rgb(217,217,217)\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 10, \"tickcolor\": \"rgb(36,36,36)\", \"ticklen\": 8, \"ticks\": \"outside\", \"tickwidth\": 2}}, \"colorscale\": {\"diverging\": [[0.0, \"rgb(103,0,31)\"], [0.1, \"rgb(178,24,43)\"], [0.2, \"rgb(214,96,77)\"], [0.3, \"rgb(244,165,130)\"], [0.4, \"rgb(253,219,199)\"], [0.5, \"rgb(247,247,247)\"], [0.6, \"rgb(209,229,240)\"], [0.7, \"rgb(146,197,222)\"], [0.8, \"rgb(67,147,195)\"], [0.9, \"rgb(33,102,172)\"], [1.0, \"rgb(5,48,97)\"]], \"sequential\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"sequentialminus\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]]}, \"colorway\": [\"#1F77B4\", \"#FF7F0E\", \"#2CA02C\", \"#D62728\", \"#9467BD\", \"#8C564B\", \"#E377C2\", \"#7F7F7F\", \"#BCBD22\", \"#17BECF\"], \"font\": {\"color\": \"rgb(36,36,36)\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"white\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"white\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"rgb(232,232,232)\", \"linecolor\": \"rgb(36,36,36)\", \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\"}, \"bgcolor\": \"white\", \"radialaxis\": {\"gridcolor\": \"rgb(232,232,232)\", \"linecolor\": \"rgb(36,36,36)\", \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"white\", \"gridcolor\": \"rgb(232,232,232)\", \"gridwidth\": 2, \"linecolor\": \"rgb(36,36,36)\", \"showbackground\": true, \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\", \"zeroline\": false, \"zerolinecolor\": \"rgb(36,36,36)\"}, \"yaxis\": {\"backgroundcolor\": \"white\", \"gridcolor\": \"rgb(232,232,232)\", \"gridwidth\": 2, \"linecolor\": \"rgb(36,36,36)\", \"showbackground\": true, \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\", \"zeroline\": false, \"zerolinecolor\": \"rgb(36,36,36)\"}, \"zaxis\": {\"backgroundcolor\": \"white\", \"gridcolor\": \"rgb(232,232,232)\", \"gridwidth\": 2, \"linecolor\": \"rgb(36,36,36)\", \"showbackground\": true, \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\", \"zeroline\": false, \"zerolinecolor\": \"rgb(36,36,36)\"}}, \"shapedefaults\": {\"fillcolor\": \"black\", \"line\": {\"width\": 0}, \"opacity\": 0.3}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"rgb(232,232,232)\", \"linecolor\": \"rgb(36,36,36)\", \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\"}, \"baxis\": {\"gridcolor\": \"rgb(232,232,232)\", \"linecolor\": \"rgb(36,36,36)\", \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\"}, \"bgcolor\": \"white\", \"caxis\": {\"gridcolor\": \"rgb(232,232,232)\", \"linecolor\": \"rgb(36,36,36)\", \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"rgb(232,232,232)\", \"linecolor\": \"rgb(36,36,36)\", \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\", \"title\": {\"standoff\": 15}, \"zeroline\": false, \"zerolinecolor\": \"rgb(36,36,36)\"}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"rgb(232,232,232)\", \"linecolor\": \"rgb(36,36,36)\", \"showgrid\": false, \"showline\": true, \"ticks\": \"outside\", \"title\": {\"standoff\": 15}, \"zeroline\": false, \"zerolinecolor\": \"rgb(36,36,36)\"}}}, \"title\": {\"text\": \"Loss function\"}, \"width\": 900, \"xaxis\": {\"title\": {\"text\": \"iter\"}}, \"yaxis\": {\"title\": {\"text\": \"train loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9620f287-af20-433c-9c4e-d6f1391fdf61');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N05YiLV2XJz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}